# -*- coding: utf-8 -*-
"""ML_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_lhh0JSA5UJrgkwfQWSrP8pp1uv621Xw
"""

#Mount Google Drive to enable access to any directory on Drive inside the Colab notebook.

from google.colab import drive
drive.mount('/content/drive')

from google.colab import data_table
data_table.enable_dataframe_formatter()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/ML Group 19

"""## **Importing libraries**"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error 
from sklearn.metrics import mean_squared_error 
from sklearn.metrics import median_absolute_error
from sklearn.model_selection import train_test_split
import pickle as pk
from sklearn.preprocessing import OneHotEncoder

"""## **Reading the dataset that was uploaded to google drive**"""

data=pd.read_csv('/content/drive/MyDrive/ML Group 19/rideshare_kaggle.csv')
data.head()

data.info

data.describe

"""## **Checking for Null values in the dataset**"""

data_=pd.DataFrame(data.isnull().sum(),columns=['Number Of Null Values'])
data_['Percentage of Null Values']=data_['Number Of Null Values']/data.shape[0]*100
data_

"""## **Data Analysis and EDA**"""

pd.DataFrame(pd.unique(data['name']),columns=['Name Of Ride'])

pd.DataFrame(pd.unique(data['cab_type']),columns=['Companies'])

data.groupby(['cab_type','name'])['price'].sum()

data_uber=data[data['cab_type']=='Uber']
print('Number Of nulls in Uber is :',data_uber['price'].isna().sum())
data_uber

data.fillna(0,inplace=True)
print('Number Of nulls in Uber is :',data['price'].isna().sum())
data

data['cab_type'].value_counts()

# Number of trips per way
data['name'].value_counts()

# Total distance of trips
data.groupby(['cab_type','name'])['distance'].sum()

# Total distance covered by each company
data.groupby(['cab_type'])['distance'].sum()

# Total price of each trips
data.groupby(['cab_type','name'])['price'].sum()

dict={'distance':[data['distance'].min(),data['distance'].max()],'price':[data['price'].min(),data['price'].max()],'date':[data['datetime'].min(),data['datetime'].max()]}
pd.DataFrame(dict,index=['minimum','maximum'])

object=data.select_dtypes(include=object).columns
pd.DataFrame(object,columns=['String Columns'])

"""## **Visuals for Data Analysis**"""

# Number of rides during different parts of the day
day=pd.cut(x=data['hour'],bins=[0,10,15,19,23],labels = ['Morning','Afternoon','Evening','Night'])
plt.title('Day',fontsize=30)
sns.countplot(x=day)
day

# Distribution of price
bins = np.linspace(data['price'].min(),data['price'].max(),100)
count=data['price'].value_counts()
plt.figure(figsize=(15,5))
plt.title('Price',fontsize=30)
sns.histplot(x='price',data=data,bins=bins)
count

# Hour of the day ride was completed
bins = np.linspace(0,24)
count=data['hour'].value_counts()
plt.figure(figsize=(15,5))
plt.title('Hours of day',fontsize=30)
sns.histplot(x='hour',data=data,bins=bins)
count

plt.figure(figsize=(35,20))
textprops = {"fontsize":15}
plt.subplot(2,3,1)
#show number of cab_type
plt.title('cab type')
plt.pie(data['cab_type'].value_counts(),labels=list(data['cab_type'].value_counts().index),autopct ='%1.3f%%',textprops=textprops)
plt.subplot(2,3,3)
# Show number of name
plt.title('Number of cab type')
plt.pie(data['name'].value_counts(),labels=list(data['name'].value_counts().index),autopct ='%1.3f%%',textprops=textprops)
plt.subplot(2,3,4)
# Show number of source
plt.title('Source location')
plt.pie(data['source'].value_counts(),labels=list(data['source'].value_counts().index),autopct ='%1.3f%%',textprops=textprops)
plt.subplot(2,3,5)
# Show number of destination
plt.title('Destination location')
plt.pie(data['destination'].value_counts(),labels=list(data['destination'].value_counts().index),autopct ='%1.3f%%',textprops=textprops)
plt.subplot(2,3,6)
# Show state of weather  
plt.title('State of Weather')
plt.pie(data['short_summary'].value_counts(),labels=list(data['short_summary'].value_counts().index),autopct ='%1.3f%%',textprops=textprops)

plt.figure(figsize=(10,10))
plt.suptitle('icon')
plt.pie(data['icon'].value_counts(),labels=list(data['icon'].value_counts().index),autopct ='%1.3f%%',textprops=textprops)

"""## **Feature Selection**"""

temp=['timestamp','datetime','long_summary','temperature','apparentTemperature','temperatureHigh','temperatureLow','apparentTemperatureHigh',
            'apparentTemperatureLow','temperatureMin','temperatureHighTime','temperatureMax','apparentTemperatureMin','apparentTemperatureMax',
             'uvIndexTime','temperatureMinTime','temperatureMaxTime','apparentTemperatureMinTime','apparentTemperatureMaxTime','price']
plt.figure(figsize=(30,15))
temp_corr=data[temp].corr()
sns.heatmap(temp_corr,annot=True,cmap='twilight_shifted',cbar=False,annot_kws={"size": 15})
temp_corr

for col in temp:
    if col=='price':
        continue
    data.drop(col,axis=1,inplace=True)
data

temp=['hour','day','month','pressure','windBearing','cloudCover','uvIndex','visibility.1','ozone','sunriseTime','sunsetTime','moonPhase','precipIntensityMax','price']
plt.figure(figsize=(30,15))
temp_corr=data[temp].corr()
sns.heatmap(temp_corr,annot=True,cmap='twilight_shifted',cbar=False,annot_kws={"size": 15})
temp_corr

for col in temp:
    if col=='price':
        continue
    data.drop(col,axis=1,inplace=True)
data

temp=['humidity','windSpeed','windGust','windGustTime','visibility','temperatureLowTime','apparentTemperatureHighTime',
      'apparentTemperatureLowTime','dewPoint','latitude','price']
plt.figure(figsize=(30,15))
temp_corr=data[temp].corr()
sns.heatmap(temp_corr,annot=True,cmap='twilight_shifted',cbar=False,annot_kws={"size": 15})
temp_corr

for col in temp:
    if col=='price':
        continue
    data.drop(col,axis=1,inplace=True)
data

temp=['precipIntensity','precipProbability','longitude','price']
sns.heatmap(data[temp].corr(),annot=True,cmap='twilight_shifted',cbar=False,annot_kws={"size": 15})
data[temp].corr()

for col in temp:
    if col=='price':
        continue
    data.drop(col,axis=1,inplace=True)
data

feature=['distance','surge_multiplier','name','cab_type']

X = data[feature]
y = data['price']
key = X.keys()

X

y

str_col=['cab_type','name']
label = LabelEncoder()
for col in str_col:
    X[col]=label.fit_transform(X[col])
X

RFModel = RandomForestRegressor(n_estimators=100,max_depth=5, random_state=33)
SelectedParameters = {'max_depth':[5,10,15,20,25]}
GridSearchModel = GridSearchCV(RFModel,SelectedParameters, cv = 2,return_train_score=True)
GridSearchModel.fit(X, y)
print('Best Score is :', GridSearchModel.best_score_)
print('Best Parameters are :', GridSearchModel.best_params_)
print('Best Estimator is :', GridSearchModel.best_estimator_)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=33, shuffle =True)

RFModel = RandomForestRegressor(n_estimators=100,max_depth=15, random_state=33)
RFModel.fit(X_train,y_train)
print('Random Forest Regressor Train Score is : ' , RFModel.score(X_train, y_train))
print('Random Forest Regressor Test Score is : ' , RFModel.score(X_test, y_test))

"""## **SVM Model**"""

from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR

scaler = StandardScaler()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=33, shuffle =True)

# Train SVM model
svm = SVR(kernel='rbf')
svm.fit(X_train, y_train)

# Predict fares for test set
y_pred = svm.predict(X_test)

# Evaluate model performance
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('Mean Squared Error for SVM:', mse)
print('R^2 Score for SVM:', r2)